# Next-word-prediction
A model that predicts the words that would follow a given sequence.It uses LSTMs to generate the next sequence.

# LSTM

![LSTM](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQHz4Ae1UDc6fK0VAuSc3HsTs4hFmiDIRcF2gnaUQbeBw&s)

Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus “long short-term memory”.

# This Model

<img width="568" alt="OUTPUTLSTM" src="https://github.com/Akkki28/Next-word-prediction/assets/120105455/e06970c1-e55d-4feb-ae31-f28958c3a875">

given the input of a sequence and the number of words to be predicted this model completes the sentence with that amount of words

